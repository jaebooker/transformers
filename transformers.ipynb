{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no transformers used in the end\n",
    "#build vocabs for Shakespeare plays\n",
    "#trained to generate shakespeare play based on data, using LSTM\n",
    "#currently used just for Hamlet. But could be used for any play, or all the plays\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These, dear onlooker, are the other models we need to import. Their true fate and purpose shall be revealed in time\n",
    "import re\n",
    "import operator\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pickle as pickle #data processing\n",
    "from collections import Counter #tokenization\n",
    "import keras #ML\n",
    "#import processing as pr #helper\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import gensim, logging\n",
    "from gensim.models import word2vec\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_db = pd.read_csv(\"shakespeare-plays/Shakespeare_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(shakespeare_db['PlayerLine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_array = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_string(play):\n",
    "    string = \"\"\n",
    "    for i in play:\n",
    "        string += i\n",
    "        string += \" \"\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alt_keywords(docs, k):\n",
    "    lines = [line.rstrip() for line in open('SmartStoplist.txt')]\n",
    "    stop_list = set(lines)\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_list)\n",
    "    tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "    tfidf_scores = np.sum(tfidf_matrix,axis=0)\n",
    "    tfidf_scores = np.ravel(tfidf_scores)\n",
    "    return sorted(dict(zip(vectorizer.get_feature_names(),tfidf_scores)).items(),key=lambda x:x[1], reverse=True)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(words):\n",
    "    #words_list = re.split(\"(?:(?:[^a-zA-Z]+')|(?:'[^a-zA-Z]+))|(?:[^a-zA-Z']+)\", words)\n",
    "    words_list = re.split(\"\\W*[^\\'\\w+\\']\", words)\n",
    "    #thanks to Martijn Pieters for the above split\n",
    "    word_dictionary = {}\n",
    "    counter = len(words_list)\n",
    "    i = 0\n",
    "    while i < counter:\n",
    "        word_dictionary[words_list[i]] = 1\n",
    "        n = i+1\n",
    "        while n < counter:\n",
    "            if words_list[i] == words_list[n]:\n",
    "                word_dictionary[words_list[i]] += 1\n",
    "                words_list.pop(n)\n",
    "                counter -= 1\n",
    "                n -= 1\n",
    "            n += 1\n",
    "        i += 1\n",
    "    return word_dictionary\n",
    "def unique_words(histogram):\n",
    "    return len(histogram)\n",
    "def frequency(word, histogram):\n",
    "    return histogram[word]\n",
    "def create_sorted_array(histogram):\n",
    "    new_array = []\n",
    "    count = 0\n",
    "    for i in histogram:\n",
    "        new_array.append([histogram[i]])\n",
    "        new_array[count].append(i)\n",
    "        count += 1\n",
    "    new_array.sort()\n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_alt_key_words(play_name, num):\n",
    "    play = shakespeare_db[shakespeare_db['Play']==play_name]\n",
    "    play = play['PlayerLine']\n",
    "    alt_keys = alt_keywords(play,num)\n",
    "    return alt_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_key_words(play_name):\n",
    "    play = shakespeare_db[shakespeare_db['Play']==play_name]\n",
    "    play = play['PlayerLine']\n",
    "    keys = keywords(play,10)\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_the_plays():\n",
    "    new_play = \"Henry IV\"\n",
    "    play_array = []\n",
    "    for i in shakespeare_db['Play']:\n",
    "        if i != new_play:\n",
    "            play_array.append(new_play)\n",
    "            new_play = i\n",
    "    return play_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_plays = all_the_plays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_keywords = [[]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_groups = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_keys():\n",
    "    for i in all_plays:\n",
    "        play_words = []\n",
    "        play_words.append(i)\n",
    "        play_words.append(find_alt_key_words(i, 50))\n",
    "        play_keywords.append(play_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['1971', 'ain', 'aren', 'cornell', 'couldn', 'cs', 'didn', 'doesn', 'don', 'english', 'ftp', 'hadn', 'hasn', 'haven', 'isn', 'list', 'mon', 'pub', 'salton', 'shouldn', 'smart', 'stop', 've', 'wasn', 'weren', 'won', 'word', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "find_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_groups(keys):\n",
    "    for i in keys:\n",
    "        play_thing = [[]]\n",
    "        for n in i:\n",
    "            if (play_thing == None) and (i[0] != None):\n",
    "                play_thing.append([i[0]])\n",
    "            if n != i[0]:\n",
    "                for y in n:\n",
    "                    for x in keys:\n",
    "                        play_match_words = []\n",
    "                        for xn in x:\n",
    "                            if xn != n:\n",
    "                                if xn != x[0]:\n",
    "                                    for xy in xn:\n",
    "                                        if xy[0] == y[0]:\n",
    "                                            play_match_words.append(xy[0])\n",
    "                            if (play_match_words != None):\n",
    "                                play_match = [[x[0]]]\n",
    "                                play_match[0].append(play_match_words)\n",
    "                                play_thing[0].append(play_match)\n",
    "        play_groups.append(play_thing)\n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_groups(play_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_groups(keys):\n",
    "    word_array = []\n",
    "    for i in keys:\n",
    "        for n in i:\n",
    "            if n != i[0]:\n",
    "                for y in n:\n",
    "                    word_in_array = False\n",
    "                    for w in word_array:\n",
    "                        if w[0] == y[0]:\n",
    "                            w.append(i[0])\n",
    "                            word_in_array = True\n",
    "                    if word_in_array != True:\n",
    "                        single_word = [y[0]]\n",
    "                        single_word.append(i[0])\n",
    "                        word_array.append(single_word)\n",
    "    return word_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = word_groups(play_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_groups(keys):\n",
    "    play_array = []\n",
    "    for i in keys:\n",
    "        array_within_array = []\n",
    "        for n in i:\n",
    "            if n == i[0]:\n",
    "                array_within_array.append(n)\n",
    "            else:\n",
    "                for y in n:\n",
    "                    for z in w:\n",
    "                        if y[0] == z[0]:\n",
    "                            for a in z:\n",
    "                                repeat = False\n",
    "                                for b in array_within_array:\n",
    "                                    if a == b[0]:\n",
    "                                        repeat = True\n",
    "                                        b[1] += 1\n",
    "                                if (repeat != True) and (a != y[0]) and (a != i[0]):\n",
    "                                    array_within_array.append([a,1])\n",
    "        play_array.append(array_within_array)\n",
    "        \n",
    "    return play_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = play_groups(play_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouping_plays(play_group, num):\n",
    "    group_array = []\n",
    "    for i in play_group:\n",
    "        a = []\n",
    "        for x in i:\n",
    "            if type(x[1]) == int:\n",
    "                if x[1] > num:\n",
    "                    if a == []:\n",
    "                        a.append(i[0])\n",
    "                    a.append(x[0])\n",
    "        if a != []:\n",
    "            group_array.append(a)\n",
    "    return group_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_keys():\n",
    "    all_plays = shakespeare_db['PlayerLine']\n",
    "    keys = keywords(all_plays,5)\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text():\n",
    "    h = shakespeare_db[shakespeare_db['Play']=='Hamlet']\n",
    "    w = h['PlayerLine']\n",
    "    text = \"\"\n",
    "    for i in w:\n",
    "        text += i\n",
    "        text += \" \"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=text.lower()\n",
    "characters = sorted(list(set(text)))\n",
    "n_to_char = {n:char for n, char in enumerate(characters)}\n",
    "char_to_n = {char:n for n, char in enumerate(characters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "length = len(text)\n",
    "seq_length = 100\n",
    "for i in range(0, length-seq_length, 1):\n",
    " sequence = text[i:i + seq_length]\n",
    " label =text[i + seq_length]\n",
    " X.append([char_to_n[char] for char in sequence])\n",
    " Y.append(char_to_n[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_modified = np.reshape(X, (len(X), seq_length, 1))\n",
    "X_modified = X_modified / float(len(characters))\n",
    "Y_modified = np_utils.to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-13 01:45:17,188 : WARNING : From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-13 01:45:18,095 : WARNING : From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "#model thanks to pranjal52\n",
    "model = Sequential()\n",
    "model.add(LSTM(400, input_shape=(X_modified.shape[1], X_modified.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(400))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(Y_modified.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-13 01:45:29,808 : WARNING : From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "163707/163707 [==============================] - 6575s 40ms/step - loss: 3.0014\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_modified, Y_modified, epochs=1, batch_size=1000)\n",
    "\n",
    "model.save_weights('text_generator_400_0.2_400_0.2_baseline.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('text_generator_400_0.2_400_0.2_baseline.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code thanks to pranjal52\n",
    "string_mapped = X[500]\n",
    "full_string = [n_to_char[value] for value in string_mapped]\n",
    "# generating characters\n",
    "for i in range(400):\n",
    "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
    "    x = x / float(len(characters))\n",
    "\n",
    "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
    "    seq = [n_to_char[value] for value in string_mapped]\n",
    "    full_string.append(n_to_char[pred_index])\n",
    "\n",
    "    string_mapped.append(pred_index)\n",
    "    string_mapped = string_mapped[1:len(string_mapped)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"e. i think i hear them. stand, ho! who's there? enter horatio and marcellus friends to this ground.                                                                                                                                                                                                                                                                                                                                                                                                                 \""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combining text, generating text in the style of Shakespeare\n",
    "txt=\"\"\n",
    "for char in full_string:\n",
    "    txt = txt+char\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
